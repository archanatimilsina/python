{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0d8349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3d4789c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mnist data\n",
    "dataset = fetch_openml('mnist_784',version=1, as_frame=False)\n",
    "X= dataset.data.astype(float)\n",
    "y= dataset.target.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f6a5ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling pixel values to 0-1\n",
    "X= X/250.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdff27e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset \n",
    "xtr, xte, ytr , yte = train_test_split(X,y, test_size=0.2, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c96aa60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(100,50),\n",
    "    solver='sgd',\n",
    "    random_state=42,\n",
    "    alpha=1e-4,\n",
    "    verbose=10,\n",
    "    max_iter=150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "907ab5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.68997972\n",
      "Iteration 2, loss = 0.75156312\n",
      "Iteration 3, loss = 0.51527564\n",
      "Iteration 4, loss = 0.42749204\n",
      "Iteration 5, loss = 0.38101537\n",
      "Iteration 6, loss = 0.35159222\n",
      "Iteration 7, loss = 0.33056790\n",
      "Iteration 8, loss = 0.31437931\n",
      "Iteration 9, loss = 0.30138403\n",
      "Iteration 10, loss = 0.28988429\n",
      "Iteration 11, loss = 0.28041968\n",
      "Iteration 12, loss = 0.27139596\n",
      "Iteration 13, loss = 0.26358454\n",
      "Iteration 14, loss = 0.25636927\n",
      "Iteration 15, loss = 0.24949164\n",
      "Iteration 16, loss = 0.24321682\n",
      "Iteration 17, loss = 0.23740416\n",
      "Iteration 18, loss = 0.23184092\n",
      "Iteration 19, loss = 0.22649856\n",
      "Iteration 20, loss = 0.22156914\n",
      "Iteration 21, loss = 0.21684403\n",
      "Iteration 22, loss = 0.21212707\n",
      "Iteration 23, loss = 0.20769325\n",
      "Iteration 24, loss = 0.20355799\n",
      "Iteration 25, loss = 0.19951447\n",
      "Iteration 26, loss = 0.19534972\n",
      "Iteration 27, loss = 0.19164434\n",
      "Iteration 28, loss = 0.18808359\n",
      "Iteration 29, loss = 0.18483998\n",
      "Iteration 30, loss = 0.18119808\n",
      "Iteration 31, loss = 0.17783697\n",
      "Iteration 32, loss = 0.17477883\n",
      "Iteration 33, loss = 0.17171741\n",
      "Iteration 34, loss = 0.16872618\n",
      "Iteration 35, loss = 0.16582235\n",
      "Iteration 36, loss = 0.16277897\n",
      "Iteration 37, loss = 0.16026060\n",
      "Iteration 38, loss = 0.15756827\n",
      "Iteration 39, loss = 0.15507574\n",
      "Iteration 40, loss = 0.15252427\n",
      "Iteration 41, loss = 0.14997841\n",
      "Iteration 42, loss = 0.14760165\n",
      "Iteration 43, loss = 0.14523053\n",
      "Iteration 44, loss = 0.14301030\n",
      "Iteration 45, loss = 0.14080110\n",
      "Iteration 46, loss = 0.13857394\n",
      "Iteration 47, loss = 0.13656373\n",
      "Iteration 48, loss = 0.13440828\n",
      "Iteration 49, loss = 0.13262565\n",
      "Iteration 50, loss = 0.13042413\n",
      "Iteration 51, loss = 0.12859535\n",
      "Iteration 52, loss = 0.12679774\n",
      "Iteration 53, loss = 0.12501964\n",
      "Iteration 54, loss = 0.12337515\n",
      "Iteration 55, loss = 0.12147070\n",
      "Iteration 56, loss = 0.11994611\n",
      "Iteration 57, loss = 0.11825066\n",
      "Iteration 58, loss = 0.11691467\n",
      "Iteration 59, loss = 0.11509679\n",
      "Iteration 60, loss = 0.11362961\n",
      "Iteration 61, loss = 0.11228054\n",
      "Iteration 62, loss = 0.11056049\n",
      "Iteration 63, loss = 0.10925626\n",
      "Iteration 64, loss = 0.10793971\n",
      "Iteration 65, loss = 0.10646637\n",
      "Iteration 66, loss = 0.10528438\n",
      "Iteration 67, loss = 0.10397329\n",
      "Iteration 68, loss = 0.10268758\n",
      "Iteration 69, loss = 0.10136062\n",
      "Iteration 70, loss = 0.10020468\n",
      "Iteration 71, loss = 0.09905591\n",
      "Iteration 72, loss = 0.09779348\n",
      "Iteration 73, loss = 0.09671859\n",
      "Iteration 74, loss = 0.09554749\n",
      "Iteration 75, loss = 0.09456600\n",
      "Iteration 76, loss = 0.09332315\n",
      "Iteration 77, loss = 0.09233078\n",
      "Iteration 78, loss = 0.09135329\n",
      "Iteration 79, loss = 0.09036869\n",
      "Iteration 80, loss = 0.08934842\n",
      "Iteration 81, loss = 0.08840330\n",
      "Iteration 82, loss = 0.08738769\n",
      "Iteration 83, loss = 0.08653984\n",
      "Iteration 84, loss = 0.08541660\n",
      "Iteration 85, loss = 0.08459766\n",
      "Iteration 86, loss = 0.08363385\n",
      "Iteration 87, loss = 0.08283668\n",
      "Iteration 88, loss = 0.08204157\n",
      "Iteration 89, loss = 0.08118311\n",
      "Iteration 90, loss = 0.08038344\n",
      "Iteration 91, loss = 0.07952862\n",
      "Iteration 92, loss = 0.07872886\n",
      "Iteration 93, loss = 0.07790467\n",
      "Iteration 94, loss = 0.07729941\n",
      "Iteration 95, loss = 0.07637360\n",
      "Iteration 96, loss = 0.07570079\n",
      "Iteration 97, loss = 0.07489497\n",
      "Iteration 98, loss = 0.07415195\n",
      "Iteration 99, loss = 0.07344394\n",
      "Iteration 100, loss = 0.07281385\n",
      "Iteration 101, loss = 0.07210362\n",
      "Iteration 102, loss = 0.07136625\n",
      "Iteration 103, loss = 0.07070006\n",
      "Iteration 104, loss = 0.07008271\n",
      "Iteration 105, loss = 0.06944651\n",
      "Iteration 106, loss = 0.06878173\n",
      "Iteration 107, loss = 0.06820004\n",
      "Iteration 108, loss = 0.06756388\n",
      "Iteration 109, loss = 0.06698251\n",
      "Iteration 110, loss = 0.06637750\n",
      "Iteration 111, loss = 0.06582127\n",
      "Iteration 112, loss = 0.06524113\n",
      "Iteration 113, loss = 0.06466160\n",
      "Iteration 114, loss = 0.06407302\n",
      "Iteration 115, loss = 0.06338791\n",
      "Iteration 116, loss = 0.06298150\n",
      "Iteration 117, loss = 0.06244090\n",
      "Iteration 118, loss = 0.06188833\n",
      "Iteration 119, loss = 0.06134342\n",
      "Iteration 120, loss = 0.06078031\n",
      "Iteration 121, loss = 0.06031614\n",
      "Iteration 122, loss = 0.05971871\n",
      "Iteration 123, loss = 0.05914663\n",
      "Iteration 124, loss = 0.05879449\n",
      "Iteration 125, loss = 0.05831050\n",
      "Iteration 126, loss = 0.05781234\n",
      "Iteration 127, loss = 0.05727988\n",
      "Iteration 128, loss = 0.05665977\n",
      "Iteration 129, loss = 0.05642311\n",
      "Iteration 130, loss = 0.05604660\n",
      "Iteration 131, loss = 0.05540488\n",
      "Iteration 132, loss = 0.05499026\n",
      "Iteration 133, loss = 0.05455040\n",
      "Iteration 134, loss = 0.05399331\n",
      "Iteration 135, loss = 0.05379444\n",
      "Iteration 136, loss = 0.05322485\n",
      "Iteration 137, loss = 0.05268193\n",
      "Iteration 138, loss = 0.05236817\n",
      "Iteration 139, loss = 0.05204498\n",
      "Iteration 140, loss = 0.05152742\n",
      "Iteration 141, loss = 0.05111566\n",
      "Iteration 142, loss = 0.05073469\n",
      "Iteration 143, loss = 0.05031165\n",
      "Iteration 144, loss = 0.04997513\n",
      "Iteration 145, loss = 0.04952138\n",
      "Iteration 146, loss = 0.04918697\n",
      "Iteration 147, loss = 0.04870862\n",
      "Iteration 148, loss = 0.04844385\n",
      "Iteration 149, loss = 0.04809611\n",
      "Iteration 150, loss = 0.04769720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/archanatimilsina/Desktop/python/.archanaArea/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# train (send x train and y train)\n",
    "mlp.fit(xtr,ytr)\n",
    "prediction = mlp.predict(xte)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "981e137d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score 97.31%\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy_score {accuracy_score(yte, prediction):.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".archanaArea",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
